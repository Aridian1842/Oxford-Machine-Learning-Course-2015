{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture2: Linear prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标\n",
    "\n",
    "1. 理解监督式学习\n",
    "2. 理解线性回归 linear regression（也就是 `最小二乘法`）\n",
    "3. 理解如何使用线性回归进行预测\n",
    "4. 学习如何推导最小二乘估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 线性监督式学习\n",
    "\n",
    "1. 许多真实过程可以通过线性模型拟合\n",
    "2. 线性回归通常作为大型系统的模块出现\n",
    "3. 线性问题可以通过分析法解决\n",
    "4. 线性预测涉及到很多机器学习中的核心概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包含 $n$ 个训练样本的（输入-输出对）数据集合：$\\{x_{1:n}, y_{1:n}\\}$，其中每一输入数据 $x \\in R^{1 \\times d}$，拥有 $d$ 个属性的向量，eg："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{1:n} = \\{x_1, x_2, ..., x_n\\}$$\n",
    "$$x_i = [x_{11}, x_{12}, ..., x_{1d}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/L2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据训练样本集合$\\{x_{1:n}, y_{1:n}\\}$我们可以得到一个模型，用于预测新的输入所对应的输出，这个过程就是**训练**或**学习**；对于新的输入值$x_{n+1}$，我们可以得到对应的预测值$ \\widehat{y}(x_{n+1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设输入与输入之间关系的模型为：$\\widehat{y}(x_i) = \\theta_1 + x_i\\theta_2$，我们可以通过**最小化目标函数**的方法来确定模型中的参数值："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\sum_{i=1}^n(y_i - \\widehat{y_i})^2 = \\sum_{i=1}^n(y_i - \\theta_1 - x_i\\theta_2)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入与输出之间呈线性关系："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\widehat{y_i} = \\sum_{j=1}^dx_{ij}\\theta_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= x_{i1}\\theta_1 + x_{i2}\\theta_2 + ... + x_{id}\\theta_d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用矩阵表示法表示：\n",
    "\n",
    "$$\\mathbf{\\widehat{y}} = \\mathbf{X\\theta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：$\\widehat{y} \\in R^{n \\times d}$，$\\mathbf{X} \\in R^{d \\times 1}$，$\\mathbf{\\theta} \\in R^{d \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/L2-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化策略\n",
    "\n",
    "最小化：\n",
    "\n",
    "$$J(\\mathbf{\\theta}) = (\\mathbf{y-X\\theta})^T(\\mathbf{y-X\\theta}) = \\sum_{i=1}^n(y_i - x_i^T\\mathbf{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/L2-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过求导求解（最小二乘法）\n",
    "\n",
    "矩阵求导规则：\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{A}\\theta}{\\partial \\mathbf{\\theta}} = \\mathbf{A^T}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{\\theta^TA\\theta}}{\\partial \\mathbf{\\theta}} = 2\\mathbf{A^T\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/L2-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面给出最小二乘法直接求解的方法，但是$\\mathbf{\\theta} = \\mathbf{(X^TX)^{-1}X^Ty}$，对矩阵求逆是一个相当费时的运算，因此一般采用迭代的方案：**梯度下降法（见附注）**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch 求解 - 线性回归的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Demo](https://github.com/torch/demos/blob/master/linear-regression/example-linear-regression.lua)**，笔记见[Blog]()。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- 0. 首先加载必须的包\n",
    "require 'torch'\n",
    "require 'optim'\n",
    "require 'nn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 10\n",
       "  3\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- 1. 训练数据\n",
    "---- 数据存储在 Torch Tensor (2维数组)，每一行（row）为一条训练样本，每一列（column）为变量（或属性）。\n",
    "---- 第一列为目标（输出），剩余为输入。\n",
    "\n",
    "data = torch.Tensor{\n",
    "   {40,  6,  4},\n",
    "   {44, 10,  4},\n",
    "   {46, 12,  5},\n",
    "   {48, 14,  7},\n",
    "   {52, 16,  9},\n",
    "   {58, 18, 12},\n",
    "   {60, 22, 14},\n",
    "   {68, 24, 20},\n",
    "   {74, 26, 21},\n",
    "   {80, 32, 24}\n",
    "}\n",
    "\n",
    "print(#data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\t\n",
       "\u001b[1;35m[flatParameters, flatGradParameters] getParameters()\u001b[0m\n",
       "\n",
       "\n",
       "This function returns two tensors. One for the flattened learnable\n",
       "parameters\u001b[0;32m flatParameters \u001b[0mand another for the gradients of the energy\n",
       "wrt to the learnable parameters\u001b[0;32m flatGradParameters \u001b[0m.\n",
       "\n",
       "Custom modules should not override this function. They should instead \n",
       "override \u001b[0mparameters(...)\u001b[0m which is, in turn, called by the present function.\n",
       "\n",
       "This function will go over all the weights and gradWeights and make them \n",
       "view into a single tensor (one for weights and one for gradWeights). Since \n",
       "the storage of every weight and gradWeight is changed, this function should \n",
       "be called only once on a given network.\t\n",
       "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\t\n",
       "\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- 2. 定义模型\n",
    "---- 这一模型（线性回归）只有一层（one layer），称为 module，两个输入属性，一个输出。\n",
    "---- 共需要3个参数：y = a0 + a1x1 + a2x2\n",
    "---- 线性模型必须储存在容器中，在多层模型中，可以采用序列容器（sequential container），\n",
    "---- 因为前一层的输出将会变成后面一层的输入。\n",
    "---- 本例中只有一层。\n",
    "\n",
    "model = nn.Sequential()                 -- 定义容器\n",
    "ninputs = 2; noutputs = 1\n",
    "model:add(nn.Linear(ninputs, noutputs)) -- 向容器中添加一个组块（层），本例中只有一个组块。\n",
    "\n",
    "-- print(help(model.forward))\n",
    "print(help(model.getParameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\t\n",
       "\u001b[1;35m[output] forward(input, target)\u001b[0m\n",
       "\n",
       "\n",
       "Given an\u001b[0;32m input \u001b[0mand a\u001b[0;32m target \u001b[0m, compute the loss function associated \n",
       "to the criterion and return the result.\n",
       "In general\u001b[0;32m input \u001b[0mand\u001b[0;32m target \u001b[0mare \u001b[0m\u001b[0;32m Tensor \u001b[0ms\u001b[0m, but some specific criterions \n",
       "might require some other type of object.\n",
       "\n",
       "The\u001b[0;32m output \u001b[0mreturned should be a scalar in general.\n",
       "\n",
       "The state variable \u001b[0m\u001b[0;32m self.output \u001b[0m\u001b[0m should be updated after a call to\u001b[0;32m \n",
       "forward() \u001b[0m.\t\n",
       "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\t\n",
       "\n",
       "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\t\n",
       "\u001b[1;35m[gradInput] backward(input, target)\u001b[0m\n",
       "\n",
       "\n",
       "Given an\u001b[0;32m input \u001b[0mand a\u001b[0;32m target \u001b[0m, compute the gradients of the loss function \n",
       "associated to the criterion and return the result.\n",
       "In general\u001b[0;32m input \u001b[0m,\u001b[0;32m target \u001b[0mand\u001b[0;32m gradInput \u001b[0mare \u001b[0m\u001b[0;32m Tensor \u001b[0ms\u001b[0m, but some \n",
       "specific criterions might require some other type of object.\n",
       "\n",
       "The state variable \u001b[0m\u001b[0;32m self.gradInput \u001b[0m\u001b[0m should be updated after a call \n",
       "to\u001b[0;32m backward() \u001b[0m.\t\n",
       "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\t\n",
       "\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- 3. 定义损失函数\n",
    "---- 最小化目标函数来确定各个参数，\n",
    "---- 本例中求得模型预测与实际值之间的`均方差`：Mean Square Error（MSE, 见：附注-2）的最小值\n",
    "\n",
    "-- tips: 可以用 help 打印函数文档，cool~\n",
    "\n",
    "criterion = nn.MSECriterion()\n",
    "\n",
    "\n",
    "print(help(criterion.forward))\n",
    "print(help(criterion.backward))\n",
    "-- print(help(optim.sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "current loss = 1.5817641763371\t\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- 4. 训练模型\n",
    "---- 通过随机梯度下降法（stochatic gradient descent procedure, SGD）求最小化损失函数的参数\n",
    "x, dl_dx = model:getParameters()\n",
    "\n",
    "-- print(x)\n",
    "-- print(dl_dx)\n",
    "\n",
    "feval = function(x_new)\n",
    "    if x ~= x_new then\n",
    "        x:copy(x_new)\n",
    "    end\n",
    "    _nidx_ = (_nidx_ or 0) + 1\n",
    "    if _nidx_ > (#data)[1] then _nidx_ = 1 end\n",
    "    \n",
    "    local sample = data[_nidx_]\n",
    "    local target = sample[{{1}}]\n",
    "    local inputs = sample[{{2, 3}}]\n",
    "    \n",
    "    dl_dx:zero()\n",
    "    local loss_x = criterion:forward(model:forward(inputs), target)\n",
    "    model:backward(inputs, criterion:backward(model.output, target))\n",
    "    \n",
    "    return loss_x, dl_dx\n",
    "end\n",
    "\n",
    "sgd_params = {\n",
    "    learningRate = 1e-3,\n",
    "    learningRateDecay = 1e-4,\n",
    "    weightDecay = 0,\n",
    "    momentum = 0\n",
    "}\n",
    "\n",
    "for i = 1, 1e4 do\n",
    "    current_loss = 0\n",
    "    for i = 1, (#data)[1] do\n",
    "        \n",
    "        -- optim.sgd@https://github.com/torch/optim/blob/master/sgd.lua\n",
    "        _, fs = optim.sgd(feval, x, sgd_params)\n",
    "        \n",
    "        current_loss = current_loss + fs[1]\n",
    "    end\n",
    "    current_loss = current_loss / (#data)[1]\n",
    "end\n",
    "print('current loss = ' .. current_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0.6667\n",
       "  1.1154\n",
       " 31.6374\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n",
       "-23.2280\n",
       "-17.4210\n",
       " -0.7259\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n",
       "\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- 5. 参数结果\n",
    "print(model:getParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\tapprox\ttext\t\n",
       " 1\t40.10\t40.32\t\n",
       " 2\t42.77\t42.92\t\n",
       " 3\t45.22\t45.33\t\n",
       " 4\t48.78\t48.85\t\n",
       " 5\t52.34\t52.37\t\n",
       " 6\t57.02\t57.00\t\n",
       " 7\t61.92\t61.82\t\n",
       " 8\t69.95\t69.78\t\n",
       " 9\t72.40\t72.19\t\n",
       "10\t79.74\t79.42\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- 6. 预测 & 测试\n",
    "test = {40.32, 42.92, 45.33, 48.85, 52.37, 57, 61.82, 69.78, 72.19, 79.42}\n",
    "print('id\\tapprox\\ttext')\n",
    "for i = 1, (#data)[1] do\n",
    "    local myPrediction = model:forward(data[i][{{2,3}}])\n",
    "    print(string.format(\"%2d\\t%.2f\\t%.2f\", i, myPrediction[1], test[i]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附注\n",
    "\n",
    "### 1. 随机梯度下降\n",
    "\n",
    "**什么是梯度下降法则？**\n",
    "\n",
    "通过求导，让参数的变化沿着_误差曲面_最陡峭的方向发展，直到达到最低点。所谓误差曲面即参数假设空间里面目标函数构成的曲面，如：\n",
    "\n",
    "![](imgs/L2-3.png)\n",
    "\n",
    "**梯度下降法的推导：**\n",
    "\n",
    "模型$h(x)$公式为：\n",
    "$$h(x) = h_\\theta(x) = \\sum_{i=1}^d\\theta_ix_i = \\vec{\\theta}^T\\vec{X}$$\n",
    "目标函数（损失函数）：\n",
    "$$J(\\vec{\\theta}) = \\frac{1}{2}\\sum_{i=1}^d(h_\\theta(x) - y_i)^2$$\n",
    "\n",
    "所谓梯度下降，是指按照$J(\\vec{\\theta})$各个维度的导数方向减少，直到目标函数到达最小值（可能是局部最小值）。在迭代过程中，对于每一个系数$\\theta_i$：\n",
    "\n",
    "$$\\theta_i = \\theta_i - \\alpha \\frac{\\partial}{\\partial \\theta_i} J(\\theta)$$\n",
    "\n",
    "对于$j \\neq i$：\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_i}J(\\theta) = (h_\\theta(x) - y)\\frac{\\partial}{\\partial \\theta_i} h_\\theta(x) = (h_\\theta(x) - y)x_i$$\n",
    "\n",
    "，于是：\n",
    "\n",
    "$$\\theta_i = \\theta_i - \\alpha (h_\\theta(x) - y)x_i$$\n",
    "\n",
    "，其中$\\alpha$为学习率（Learning Rate）。\n",
    "\n",
    "**梯度下降算法：**\n",
    "\n",
    "```lua\n",
    "GD(traning_examples, lr) -- lr is learning rate\n",
    "    training_examples 是 {inputs, output}\n",
    "    初始化 w_i 为某个随机值\n",
    "    终止条件之前，迭代：\n",
    "        初始化 delta_w_i = 0\n",
    "        对于 training_samples 中每一条样本数据，do\n",
    "            将 inputs 输入，计算输出为 o\n",
    "            对于每个 w_i，do\n",
    "                delta_w_i = delta_w_i + lr*(output - o)*x_i -- step A\n",
    "        对于每个 w_i，do\n",
    "            w_i = w_i + delta_w_i                      -- step B\n",
    "```\n",
    "> SGD 算法，去掉 `step B`，并将 `step A` 替换为 `w_i = w_i + lr * (output - o) * x_i`。\n",
    "\n",
    "#### 2. MSE\n",
    "\n",
    "均方差（MSE）：https://en.wikipedia.org/wiki/Mean_squared_error\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat{Y_i} - Y_i)^2$$\n",
    "\n",
    "#### 3. 向量的导数 & 偏导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考链接\n",
    "\n",
    "1. [知乎：最小二乘法和梯度下降法有哪些区别？](https://www.zhihu.com/question/20822481)\n",
    "2. [机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)](http://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
