{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标\n",
    "\n",
    "1. 推导岭回归（ridge regression）\n",
    "2. 理解数据拟合和正则化（regularizing）的权衡\n",
    "3. 学习多元回归\n",
    "4. 理解在基函数给定的条件下，学习参数的问题仍然是线性的\n",
    "5. 学习交叉检验\n",
    "6. 理解模型的复杂度和 泛化（generalization）\n",
    "\n",
    "### 正则化\n",
    "\n",
    "什么是正则化？\n",
    "\n",
    "> Q1. Explain what regularization is and why it is useful.\n",
    "\n",
    "> Answer by Matthew Mayo. \n",
    "\n",
    "> Regularization is the process of adding a tuning parameter to a model to induce smoothness in order to prevent overfitting. (see also KDnuggets posts on Overfitting) \n",
    "\n",
    "\n",
    "> This is most often done by adding a constant multiple to an existing weight vector. This constant is often either the L1 (Lasso) or L2 (ridge), but can in actuality can be any norm. The model predictions should then minimize the mean of the loss function calculated on the regularized training set. \n",
    "\n",
    "> Xavier Amatriain presents a good [comparison of L1 and L2 regularization](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization) here, for those interested. \n",
    "\n",
    "到目前为止所有问题的解都是如下形式（最小二乘法）：\n",
    "\n",
    "$$ \\hat{\\theta} = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 最小二乘法推导（见L2）\n",
    "\n",
    "> 在回归分析中最小二乘法是最常用的方法，使用最小二乘法的一个前提是$|X^TX|$不为零，即矩阵$X^TX$非奇异，当所有变量之间有较强的线性相关性时，或者变量之间的数据变化比较小或者部分变量之间有线性相关性时，矩阵$X^TX$的行列式比较小，甚至趋近于0，一般在实际应用中处理：当<0.01时常被称为病态矩阵，它表明最小二乘法并非在各方面都尽善尽美，因为这种矩阵在计算过程中极易造成约数误差，因此得到的数据往往缺乏稳定性和可靠性。\n",
    "\n",
    "> 岭回归是在自变量信息矩阵的主对角线元素上人为地加入一个非负因子，从而使回归系数的估计稍有偏差、而估计的稳定性却可能明显提高的一种回归分析方法，它是最小二乘法的一种补充，岭回归可以修复病态矩阵，达到较好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推导\n",
    "\n",
    "![L4-1](imgs/L4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考\n",
    "\n",
    "1. [Kernel Ridge Regression](http://stat.wikia.com/wiki/Kernel_Ridge_Regression)\n",
    "2. [Coursera公开课笔记: 斯坦福大学机器学习第七课“正则化(Regularization)”](http://52opencourse.com/133/coursera%E5%85%AC%E5%B)\n",
    "3. [21 Must-Know Data Science Interview Questions and Answers](http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
